{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# Lab 4: Document Understanding with Agentic Document Extraction II\n",
    "\n",
    "In this lab, you will process multiple documents, categorize their types, and extract specific fields according to their respective schemas.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Specify extraction schemas with Pydantic \n",
    "- Implement categorization of documents by type\n",
    "- Build validation logic into extracted information\n",
    "\n",
    "## Background\n",
    "\n",
    "Banks receive loan application documents with arbitrary filenames (eg \"uploadA.pdf\", \"image456.jpg\"). The workflow must:\n",
    "1. Identify each document type (pay stub, W2, bank statement, etc.)\n",
    "2. Extract relevant fields based on a schema pertaining to that document type\n",
    "3. Validate that all documents belong to the same applicant\n",
    "\n",
    "## Outline\n",
    "\n",
    "- [1. Setup and Authentication](#1)\n",
    "- [2. Helper Functions](#2)\n",
    "- [3. Full Document Processing Pipeline](#3)\n",
    "  - [3.1 Preview User-Supplied Documents](#3-1)\n",
    "  - [3.2 Document Categorization Schema](#3-2)\n",
    "  - [3.3 Document-Specific Extraction Schemas](#3-3)\n",
    "  - [3.4 Parse and Categorize Documents](#3-4)\n",
    "  - [3.5 Extract Financial Data](#3-5)\n",
    "  - [3.6 Visualize Parsing Results](#3-6)\n",
    "  - [3.7 Visualize Extracted Fields Only](#3-7)\n",
    "  - [3.8 Create a Final Dataframe](#3-8)\n",
    "  - [3.9 Validation Logic](#3-9)\n",
    "- [4. Summary](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-1",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1. Setup and Authentication\n",
    "\n",
    "Import the required libraries and initialize the ADE client. This setup is identical to the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f449249-2919-4835-8777-7457b5ef679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import os\n",
    "import json\n",
    "import pymupdf\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, IFrame, Markdown, HTML\n",
    "from IPython.display import Image as DisplayImage\n",
    "from PIL import Image as PILImage, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effecd22-78f9-48ba-90f0-62d4f9d9b597",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports specific to Agentic Document Extraction\n",
    "from landingai_ade import LandingAIADE\n",
    "from landingai_ade.types import ParseResponse, ExtractResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7cb740-e216-4a5b-9673-74d87edb1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env\n",
    "_ = load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e11fa7-0c7b-40fa-81cc-40192d26b54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the client\n",
    "client = LandingAIADE()\n",
    "print(\"Authenticated client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-2",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2. Helper Functions\n",
    "\n",
    "Import visualization helpers for displaying documents and bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109dca29-3fe4-4830-bcb1-32f5431e1084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import print_document, draw_bounding_boxes, draw_bounding_boxes_2\n",
    "from helper import create_cropped_chunk_images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3. Full Document Processing Pipeline: Loan Automation\n",
    "\n",
    "Imagine you work at a bank reviewing loan applications. Applicants upload various financial documents with arbitrary names. Your pipeline needs to:\n",
    "\n",
    "1. **Parse** all documents to understand their content\n",
    "2. **Categorize** each document (Is it a pay stub? Bank statement? ID?)\n",
    "3. **Extract** the relevant fields based on document type\n",
    "4. **Validate** that all documents belong to the same person"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-1",
   "metadata": {},
   "source": [
    "<a id=\"3-1\"></a>\n",
    "\n",
    "### 3.1 Preview the User-Supplied Documents\n",
    "\n",
    "Preview all documents in the input folder. These are sample documents from different people (for demonstration purposes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5298300-5526-42c7-915f-6bcd7874521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_directory(directory_path: str):\n",
    "    \"\"\"\n",
    "    Displays all supported documents (PDFs and images) in a directory.\n",
    "\n",
    "    Args:\n",
    "        directory_path: Path to the directory containing the documents.\n",
    "    \"\"\"\n",
    "    directory = Path(directory_path)\n",
    "\n",
    "    if not directory.exists() or not directory.is_dir():\n",
    "        print(f\"Directory not found: {directory_path}\")\n",
    "        return\n",
    "\n",
    "    # Supported extensions\n",
    "    supported = {\".png\", \".jpg\", \".jpeg\", \".pdf\"}\n",
    "\n",
    "    # Get all matching files\n",
    "    files = sorted([f for f in directory.iterdir()\n",
    "                    if f.suffix.lower() in supported])\n",
    "\n",
    "    if not files:\n",
    "        print(\"No supported documents found.\")\n",
    "        return\n",
    "\n",
    "    # Display each document using your existing helper\n",
    "    for f in files:\n",
    "        print(f\"\\n--- Displaying: {f.name} ---\\n\")\n",
    "        print_document(str(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9942190-a7d6-41a3-87e2-36028ad86b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_directory(\"input_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-2",
   "metadata": {},
   "source": [
    "<a id=\"3-2\"></a>\n",
    "\n",
    "### 3.2 Document Categorization Schema\n",
    "\n",
    "This lab uses **Pydantic** instead of JSON to define schemas. Pydantic provides type validation, rich descriptions via `Field()`, and enum support for constrained values.\n",
    "\n",
    "ADE accepts both JSON and Pydantic schemas, converting Pydantic to JSON before sending to the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c07d00-2a68-44e8-82b5-9ebcc7fcc39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Pydantic for schema definition\n",
    "# ADE accepts pydantic and JSON schemas.\n",
    "# Exercise 1 used JSON. Exercise 3 uses pydantic.\n",
    "\n",
    "from enum import Enum\n",
    "from pydantic import BaseModel, Field\n",
    "from landingai_ade.lib import pydantic_to_json_schema\n",
    "\n",
    "class DocumentType(str, Enum):\n",
    "    ID = \"ID\"\n",
    "    W2 = \"W2\"\n",
    "    pay_stub = \"pay_stub\"\n",
    "    bank_statement = \"bank_statement\"\n",
    "    investment_statement = \"investment_statement\"\n",
    "\n",
    "    # Descriptions for each value\n",
    "    def describe(self) -> str:\n",
    "        descriptions = {\n",
    "            \"ID\": \"An official government identification such as a \"\n",
    "            \"passport or driver's license.\",\n",
    "            \"W2\": \"A year-end W-2 form reporting annual taxable wages \"\n",
    "            \"and withholdings.\",\n",
    "            \"pay_stub\": \"A periodic employee earnings statement.\",\n",
    "            \"bank_statement\": \"A checking or savings account statement \"\n",
    "            \"with balances and transactions.\",\n",
    "            \"investment_statement\": \"A brokerage or investment account \"\n",
    "            \"statement showing holdings, value, and transactions.\",\n",
    "        }\n",
    "        return descriptions[self.value]\n",
    "\n",
    "class DocType(BaseModel):\n",
    "    type: DocumentType = Field(\n",
    "        description=\"The type of document being analyzed.\",\n",
    "        title=\"Document Type\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-3",
   "metadata": {},
   "source": [
    "<a id=\"3-3\"></a>\n",
    "\n",
    "### 3.3 Define Document-Specific Extraction Schemas\n",
    "\n",
    "Each document type has different fields of interest. Define separate Pydantic schemas for:\n",
    "- **ID**: Name, issuer, issue date, identifier\n",
    "- **W2**: Employee/employer names, year, wages (Box 1)\n",
    "- **Pay Stub**: Employee/employer names, pay period, gross/net pay\n",
    "- **Bank Statement**: Account owner, bank name, account number, balance\n",
    "- **Investment Statement**: Account owner, institution, year, total value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4134a1b-b7bc-411b-b9b5-504f36ab6856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# Schema for ID\n",
    "# ---------------------------------------------------------\n",
    "class IDSchema(BaseModel):\n",
    "    name: str = Field(description=\"Full name of the person\", \n",
    "                      title=\"Full Name\")\n",
    "    issuer: str = Field(description=\"The state or country issuing the \"\n",
    "                        \"identification.\", title=\"Issuer\")\n",
    "    issue_date: str = Field(description=\"The issue date for the \"\n",
    "                            \"identification.\", title=\"Issue Date\")\n",
    "    identifier: str = Field(description=\"The unique identifier such as a \"\n",
    "                            \"drivers license number or passport number\", \n",
    "                            title=\"Identifier\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Schema for W2\n",
    "# ---------------------------------------------------------\n",
    "class W2Schema(BaseModel):\n",
    "    employee_name: str = Field(description=\"The name of the employee.\", \n",
    "                               title=\"Employee Name\")\n",
    "    employer_name: str = Field(description=\"The name of the employer \"\n",
    "                               \"organization issuing the W2.\", \n",
    "                               title=\"Employer Name\")\n",
    "    w2_year: int = Field(description=\"The year of the W2 form.\", \n",
    "                         title=\"W2 Year\")\n",
    "    wages_box_1: float = Field(description=\"The total wages shown in box 1 \"\n",
    "                               \"of the form\", title=\"Box 1\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Schema for Pay Stubs\n",
    "# ---------------------------------------------------------\n",
    "class PaymentStubSchema(BaseModel):\n",
    "    employee_name: str = Field(description=\"The name of the employee.\", \n",
    "                               title=\"Employee Name\")\n",
    "    employer_name: str = Field(description=\"The name of the employer \"\n",
    "                               \"organization.\", title=\"Employer Name\")\n",
    "    pay_period: str = Field(description=\"The pay period for the stub.\",\n",
    "                            title=\"Pay Period\")\n",
    "    gross_pay: float = Field(description=\"The gross pay amount.\",\n",
    "                             title=\"Gross Pay\")\n",
    "    net_pay: float = Field(description=\"The net pay amount after \"\n",
    "                           \"deductions.\", title=\"Net Pay\")\n",
    "    \n",
    "# ---------------------------------------------------------\n",
    "# Schema for Bank Statements\n",
    "# ---------------------------------------------------------\n",
    "class BankStatementSchema(BaseModel):\n",
    "    account_owner: str = Field(description=\"The name of the account \"\n",
    "                               \"owner(s).\", title=\"Account Owner\")\n",
    "    bank_name: str = Field(description=\"The name of the bank.\", \n",
    "                           title=\"Bank Name\")\n",
    "    account_number: str = Field(description=\"The bank account number.\", \n",
    "                                title=\"Account Number\")\n",
    "    end_date: str = Field(description=\"The ending date for the statement.\", \n",
    "                          title=\"End Date\")\n",
    "    balance: float = Field(description=\"The current balance of the bank \"\n",
    "                           \"account.\", title=\"Bank Balance\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Schema for Investment Statements\n",
    "# ---------------------------------------------------------\n",
    "class InvestmentStatementSchema(BaseModel):\n",
    "    account_owner: str = Field(description=\"The name of the account owner(s).\"\n",
    "                               , title=\"Account Owner\")\n",
    "    institution_name: str = Field(description=\"The name of the financial \"\n",
    "                                  \"institution.\", title=\"Institution Name\")\n",
    "    investment_year: int = Field(description=\"The year of the investment \"\n",
    "                                 \"statement.\", title=\"Investment Year\")\n",
    "    investment_value: float = Field(description=\"The total value of the \"\n",
    "                                    \"account as of the statement end date.\", \n",
    "                                    title=\"Investment Balance\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Map document types to their corresponding schemas\n",
    "# ---------------------------------------------------------\n",
    "schema_per_doc_type = {\n",
    "    \"bank_statement\": BankStatementSchema,\n",
    "    \"investment_statement\": InvestmentStatementSchema,\n",
    "    \"pay_stub\": PaymentStubSchema,\n",
    "    \"ID\": IDSchema,\n",
    "    \"W2\": W2Schema,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3e7f99-6993-4784-953d-f574e796a657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the document type schema to JSON format for API calls\n",
    "doc_type_json_schema = pydantic_to_json_schema(DocType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-4",
   "metadata": {},
   "source": [
    "<a id=\"3-4\"></a>\n",
    "\n",
    "### 3.4 Parse and Categorize Documents\n",
    "\n",
    "For each document:\n",
    "1. **Parse** to extract content (using `split=\"page\"` for per-page markdown)\n",
    "2. **Categorize** using only the first page (sufficient for identification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d9b1f-2af5-48d2-84b7-f35b415802fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = Path(\"input_folder\")\n",
    "\n",
    "# Dictionary to store document types and parse results\n",
    "document_types = {}\n",
    "\n",
    "# Process each document in the folder\n",
    "for document in input_folder.iterdir():\n",
    "\n",
    "    # üî• Skip directories so ADE doesn't try to parse them\n",
    "    if document.is_dir():\n",
    "        continue\n",
    "        \n",
    "    print(f\"Processing document: {document.name}\")\n",
    "\n",
    "    # Step 1: Parse the document to extract layout and content\n",
    "    parse_result: ParseResponse = client.parse(\n",
    "        document=document,\n",
    "        split=\"page\",  #Notice that each document is being split by page.\n",
    "        model=\"dpt-2-latest\"\n",
    "    )\n",
    "    print(\"Parsing completed.\")\n",
    "    print(\" \")\n",
    "    \n",
    "    # Notice that we only use the first page to determine the document type\n",
    "    first_page_markdown = parse_result.splits[0].markdown  \n",
    "    \n",
    "    # Step 2: Extract document type using the categorization schema\n",
    "    print(\"Extracting Document Type...\")\n",
    "    extraction_result: ExtractResponse = client.extract(\n",
    "        schema=doc_type_json_schema,\n",
    "        markdown=first_page_markdown\n",
    "    )\n",
    "    doc_type = extraction_result.extraction[\"type\"]\n",
    "    print(f\"Document Type Extraction: {doc_type}\\n\")\n",
    "    print(\"       ----------         \")\n",
    "    print(\" \")\n",
    "    \n",
    "    # Store results for later use\n",
    "    document_types[document] = {\n",
    "        \"document_type\": doc_type,\n",
    "        \"parse_result\": parse_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-5",
   "metadata": {},
   "source": [
    "<a id=\"3-5\"></a>\n",
    "\n",
    "### 3.5 Extract Financial Data Based on Document Type\n",
    "\n",
    "Now that we know each document's type, we apply the appropriate schema to extract the relevant fields. The schema mapping (`schema_per_doc_type`) ensures each document gets the correct extraction template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbb19af-b556-40a4-a48d-f99aa5e29ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store extraction results\n",
    "document_extractions = {}\n",
    "\n",
    "# Extract financial data from each document using its specific schema\n",
    "for document, extraction in document_types.items():\n",
    "    print(f\"Processing document: {document.name}\")\n",
    "\n",
    "    # Get the appropriate schema for this document type\n",
    "    json_schema = pydantic_to_json_schema(\n",
    "        schema_per_doc_type[extraction[\"document_type\"]]\n",
    "    )\n",
    "\n",
    "    # Extract structured data using the schema\n",
    "    extraction_result: ExtractResponse = client.extract(\n",
    "        schema=json_schema,\n",
    "        markdown=extraction[\"parse_result\"].markdown\n",
    "    )\n",
    "    print(\"Detailed Extraction:\", extraction_result.extraction)\n",
    "\n",
    "    # Store extraction results\n",
    "    document_extractions[document] = {\n",
    "        \"extraction\": extraction_result.extraction,\n",
    "        \"extraction_metadata\": extraction_result.extraction_metadata,\n",
    "    }\n",
    "\n",
    "print(document_extractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-6",
   "metadata": {},
   "source": [
    "<a id=\"3-6\"></a>\n",
    "\n",
    "### 3.6 Visualize Parsing Results with Bounding Boxes\n",
    "\n",
    "Visualize all detected chunks for each document to verify parsing quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8b80c6-12f3-417b-a625-b1b9851bf59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import draw_bounding_boxes_2\n",
    "\n",
    "# Combine all extraction data\n",
    "final_extractions = {}\n",
    "\n",
    "for document, extraction in document_extractions.items():\n",
    "    final_extractions[document] = {\n",
    "        **extraction,\n",
    "        **document_types[document],\n",
    "    }\n",
    "\n",
    "# Visualize all parsed chunks for each document\n",
    "for document, extraction in final_extractions.items():\n",
    "    print(f\"Visualizing document: {document.name}\")\n",
    "    base_path = f\"results/{document.stem}\"\n",
    "    os.makedirs(base_path, exist_ok=True)\n",
    "    draw_bounding_boxes_2(\n",
    "        extraction[\"parse_result\"].grounding,\n",
    "        document,\n",
    "        base_path=base_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef671ec-2c58-43af-8b86-bb0502ade0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PILImage.open(f\"results/uploadC/page_1_annotated.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b503291-4266-48d9-9b41-692e1cb4e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "PILImage.open(f\"results/uploadE/page_1_annotated.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-7",
   "metadata": {},
   "source": [
    "<a id=\"3-7\"></a>\n",
    "\n",
    "### 3.7 Visualize Extracted Fields Only\n",
    "\n",
    "For human-in-the-loop systems, highlight only the extracted fields to show reviewers where values originated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7685d-c361-477c-b469-ebd271784547",
   "metadata": {},
   "outputs": [],
   "source": [
    "for document, extraction in final_extractions.items():\n",
    "    print(f\"Visualizing extracted fields for: {document.name}\")\n",
    "    base_path = f\"results_extracted/{document.stem}\"\n",
    "\n",
    "    parse_result = extraction[\"parse_result\"]\n",
    "    document_grounds = {}\n",
    "\n",
    "    for label, metadata_value in extraction[\"extraction_metadata\"].items():\n",
    "        chunk_id = metadata_value[\"references\"][0]\n",
    "        grounding = parse_result.grounding[chunk_id]\n",
    "        document_grounds[chunk_id] = grounding\n",
    "\n",
    "    draw_bounding_boxes_2(\n",
    "        document_grounds,  # dict of chunk_id -> grounding\n",
    "        document,\n",
    "        base_path=base_path\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6925d80b-b241-4db8-a8d4-7d213ce6d849",
   "metadata": {},
   "outputs": [],
   "source": [
    "PILImage.open(f\"results_extracted/uploadC/page_1_annotated.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-8",
   "metadata": {},
   "source": [
    "<a id=\"3-8\"></a>\n",
    "\n",
    "### 3.8 Create a Final Dataframe\n",
    "\n",
    "Consolidate all extracted fields into a summary dataframe for a complete view of applicant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a8107-89f5-487d-974d-0408f35a7f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Collect all the fields into a summary dataframe\n",
    "rows = []\n",
    "\n",
    "for document, info in document_extractions.items():\n",
    "    extraction = info[\"extraction\"]\n",
    "    doc_type = document_types[document][\"document_type\"]  # from your classification step\n",
    "\n",
    "    input_folder = document.parent.name\n",
    "    document_name = document.name\n",
    "\n",
    "    for field, value in extraction.items():\n",
    "        rows.append({\n",
    "            \"applicant_folder\": input_folder,\n",
    "            \"document_name\": document_name,\n",
    "            \"document_type\": doc_type,\n",
    "            \"field\": field,\n",
    "            \"value\": value,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-3-9",
   "metadata": {},
   "source": [
    "<a id=\"3-9\"></a>\n",
    "\n",
    "### 3.9 Validation Logic\n",
    "\n",
    "Apply business logic to validate the submission:\n",
    "- **Name matching**: Verify all documents belong to the same person\n",
    "- **Year verification**: Check all documents are from recent years\n",
    "- **Asset totals**: Calculate the applicant's total net worth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "name-check",
   "metadata": {},
   "source": [
    "**Check 1: Name Matching**\n",
    "\n",
    "Verify that all name fields across documents match to catch mismatched submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5e2f76-4461-4fa2-9827-d6ab81d03b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic check to determine whether the five name fields extracted \n",
    "# from five documents match each other.\n",
    "\n",
    "name_fields = {\"account_owner\", \"employee_name\", \"name\"}\n",
    "df_names = df[df[\"field\"].isin(name_fields)].copy()\n",
    "all_names_match = df_names[\"value\"].nunique() == 1\n",
    "\n",
    "if all_names_match:\n",
    "    print(\"‚úÖ All 5 name fields match!\")\n",
    "else:\n",
    "    print(\"‚ùå The name fields do NOT match.\")\n",
    "    print(\"Values found:\")\n",
    "    print(df_names[[\"document_name\", \"field\", \"value\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "year-check",
   "metadata": {},
   "source": [
    "**Check 2: Document Year**\n",
    "\n",
    "Loan applications require recent documents. Extract and verify the year from each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa3575c-f62a-4ed3-9866-621372c4a3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to extract and check the year associated with each document\n",
    "\n",
    "import re\n",
    "\n",
    "# 1. Fields that may contain a year\n",
    "year_fields = {\n",
    "    \"w2_year\",\n",
    "    \"investment_year\",\n",
    "    \"issue_date\",\n",
    "    \"end_date\",\n",
    "    \"pay_period\",\n",
    "}\n",
    "\n",
    "# 2. Helper to pull a 4-digit year out of a string/number\n",
    "def extract_year(value):\n",
    "    \"\"\"\n",
    "    Return a 4-digit year (1900‚Äì2099) from a value, or None if none found.\n",
    "    \"\"\"\n",
    "    if value is None:\n",
    "        return None\n",
    "    match = re.search(r\"\\b(19|20)\\d{2}\\b\", str(value))\n",
    "    return int(match.group(0)) if match else None\n",
    "\n",
    "\n",
    "# 3. Build a table of years per document\n",
    "year_rows = []\n",
    "\n",
    "for doc_name in df[\"document_name\"].unique():\n",
    "    doc_df = df[df[\"document_name\"] == doc_name]\n",
    "\n",
    "    # Only rows whose field is one of our year-related fields\n",
    "    doc_year_fields = doc_df[doc_df[\"field\"].isin(year_fields)]\n",
    "\n",
    "    for _, row in doc_year_fields.iterrows():\n",
    "        year_value = extract_year(row[\"value\"])\n",
    "        year_rows.append({\n",
    "            \"document_name\": doc_name,\n",
    "            \"field\": row[\"field\"],\n",
    "            \"value\": row[\"value\"],\n",
    "            \"year_extracted\": year_value,\n",
    "        })\n",
    "\n",
    "df_years = pd.DataFrame(year_rows)\n",
    "\n",
    "print(\"Per-document year info:\")\n",
    "print(df_years)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asset-check",
   "metadata": {},
   "source": [
    "**Check 3: Total Assets**\n",
    "\n",
    "Sum all bank and investment balances to determine loan eligibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d88694-cebf-42ea-ab1d-ca4782f4870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logic to sum all bank balances and all investment balances from your extraction\n",
    "\n",
    "# Define fields\n",
    "bank_balance_field = \"balance\"\n",
    "investment_balance_field = \"investment_value\"\n",
    "\n",
    "# Filter rows\n",
    "df_bank = df[df[\"field\"] == bank_balance_field].copy()\n",
    "df_invest = df[df[\"field\"] == investment_balance_field].copy()\n",
    "\n",
    "# Ensure numeric\n",
    "df_bank[\"value\"] = pd.to_numeric(df_bank[\"value\"], errors=\"coerce\")\n",
    "df_invest[\"value\"] = pd.to_numeric(df_invest[\"value\"], errors=\"coerce\")\n",
    "\n",
    "# Compute totals\n",
    "total_bank = df_bank[\"value\"].sum()\n",
    "total_investments = df_invest[\"value\"].sum()\n",
    "total_assets = total_bank + total_investments\n",
    "\n",
    "# Print\n",
    "print(f\"Total Bank Balances: ${total_bank:,.2f}\")\n",
    "print(f\"Total Investment Balances: ${total_investments:,.2f}\")\n",
    "print(f\"Total Assets: ${total_assets:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-4",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "In this lab, you built a document processing pipeline for loan automation:\n",
    "\n",
    "| Step | Description |\n",
    "|------|-------------|\n",
    "| **Parse** | Convert documents to structured markdown with chunks and grounding |\n",
    "| **Categorize** | Identify document types from the first page according to a categorization schema |\n",
    "| **Extract** | Apply Pydantic schemas specific to document type |\n",
    "| **Visualize** | Display bounding boxes for all chunks  |\n",
    "| **Validate** | Apply data validation to fields (eg name matching, year checks, asset totals) |\n",
    "\n",
    "Our approach applies to other scenarios such as insurance claims, healthcare records, legal briefings, and payroll processing.\n",
    "\n",
    "In the next lesson, you'll use ADE for RAG (Retrieval-Augmented Generation) applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "L1L5",
   "language": "python",
   "name": "l1l5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
